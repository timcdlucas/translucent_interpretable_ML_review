\documentclass[12pt,]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\usepackage{subfig}

\usepackage[usenames,dvipsnames]{color}
\definecolor{boxcolour}{rgb}{1,0.98,0.8}

\usepackage{geometry}
%\geometry{verbose,a4paper,tmargin=27mm,bmargin=25mm,lmargin=25mm,rmargin=25mm}

\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi

\usepackage{mathptmx}
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={A translucent box: interpretable machine learning in ecology},
            pdfauthor={Tim C. D. Lucas},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{plainnat}

\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\linespread{2}
\raggedright
\usepackage{lineno}
\linenumbers

\usepackage[font=footnotesize]{caption}


%\setcounter{topnumber}{1}
%\setcounter{totalnumber}{1}
%\renewcommand{\topfraction}{0.6}
%\renewcommand{\textfraction}{0.3}

\title{A translucent box: interpretable machine learning in ecology}
\author{Tim C. D. Lucas\textsuperscript{1}\\
1. Big Data Institute, University of Oxford}

\date{}

\begin{document}
\maketitle

Running header: A translucent box

\section{Abstract}\label{abstract}

Machine learning has become popular in ecology but its use has remained restricted to predicting, rather than understanding, the natural world.
Many researchers consider machine learning algorithms to be a black box.
These models can however, with careful examination, be used to inform our understanding of the world.
They are translucent boxes.
Furthermore, the interpretation of these models can be an important step in building confidence in a model or in a specific prediction from a model.
Here I review a number of techniques for interpreting machine learning models at the level of the system, the variable and the individual prediction as well as methods for handling non-independent data.
I also discuss the limits of interpretability for different methods and demonstrate these approaches using a case example of understanding litter sizes in mammals.




\section{Introduction}\label{introduction}

\subsection{Machine learning in ecology}\label{machine-learning-in-ecology}

Machine learning methods are a collection of techniques that focus on making accurate predictions from data \citep{crisci2012review, breiman2001statistical, domingos}.
It differs from the broader field of statistics in two aspects: 1) the estimation of parameters that relate to the real world is less emphasised and 2) the driver of the predictions are expected to be the data rather than expert opinion and careful selection of plausible mechanistic models \citep{breiman2001statistical, domingos}.
High-level machine learning libraries that aid the full machine learning pipeline \citep{caret, scikit, maxent, biomod} have made machine learning easy to use.
These techniques have therefore become popular, particularly in the fields of species distribution modelling \citep{maxent, biomod, elith2006novel, golding2018zoon, gobeyn2019evolutionary} and species identification from images or acoustic detectors \citep{mac2018bat, waldchen2018machine, shamir2014classification, xue2017automatic, fairbrass2019citynet}.
Other uses include any study where prediction, rather than inference, is the focus such as predicting the conservation status of species \citep{bland2015predicting} and predicting behavioural states \citep{browning2018predicting}.
However, machine learning methods have a reputation for being a black box, inscrutable and mindlessly applied.

This reputation is not totally unfounded with a number of factors making machine learning models difficult to interpret.
Firstly, they are often nonparametric.
They therefore estimate nonlinear relationships between covariates and response variables which are not summarised by a small number of interpretable parameters instead containing huge numbers of parameters often without estimates of uncertainty \citep{domingos}.
Secondly, they often fit deep interactions between covariates \citep{lunetta2004screening}.
Even simple, two-way interactions in linear models cause confusion \citep{engqvist2005mistreatment, lamina2012visualizing} and deep, nonlinear interactions are difficult to visualise or understand.
Thirdly, fitting machine learning models is often stochastic \citep{breiman2001random, glorot2002understanding} and fitting the same model with different starting values may give a totally different set of fitted parameters (though perhaps with similar predictive performance).
However, while interpreting machine learning models can be difficult, there is plenty of insight to be gained by appraising these models at the level of the system, the variable and the individual prediction as well as by considering autocorrelative structure in the data.

Given the black box reputation one might wonder why we should bother interpreting machine learning models; if the predictions are good, then the objective has been achieved.
However, any predictions that may be used to make decisions (i.e.~any predictions of interest) should be examined carefully.
Particular examples of this include predictions used for conservation policy or health care \citep{vayena2018machine}.
Careless predictions can have severe effects on the entity for which the predictions are being made (an endangered species or a person at risk of a disease for example) and can more generally erode trust between modellers, policy makers and other stakeholders.
In regulated fields such as healthcare, these considerations come with legal backing.
Appraising models as part of model verification has been the primary driver of research in the field of interpretable machine learning \citep{molnar, ribeiro2016should}.
Broadly we might want to carefully quantify how accurate the predictions from our models are, consider whether the fitted relationships between covariates and response variables are plausible and understand why an individual prediction is given.


However, there are further reasons to interpret machine learning models that apply to fields that are further removed from policy decisions \citep{elith2009species}.
The same traits that make machine learning models good at prediction and difficult to interpret also makes them potentially useful in exploratory analysis before more formal statistical modeling \citep{zhao2017causal, gelman2014statistical, nosek2012scientific}.
Firstly, they can be useful as a global-level baseline to compare how well a mechanistic model performs.
Secondly, the nonparametric nature of many machine learning models means they can discover nonlinear relationships between covariates and response variables and interactions between covariates without specifying them \emph{a priori} as would be required in more statistical modelling.
It is also worth noting that standard statistical models are often not as interpretable as they seem; understanding the results from a statistical model is made more difficult in the presence of collinearity between covariates or when nature's true model is not in the set of models being considered \citep{lyddon2018nonparametric, yao2017using}.
Therefore, in some cases it might be better to fit a more predictive model and sacrifice some, but not all, interpretability.
Alternatively, it might be useful to use a highly predictive model to generate hypotheses which could then be tested in a more formal statistical framework \citep{zhao2017causal}.


\subsection{An overview of machine learning}\label{an-overview-of-machine-learning}

Throughout this paper, the working definition of machine learning is any model where the focus is prediction rather than the building models that directly reflect reality.
Supervised learning is a subfield of machine learning and is the archetypal modelling found in biology.
The analyst has some response data and some covariates and the task is to predict the response data.
Therefore models such as generalised linear models, mixed effects models and time series modelling would come under supervised learning.
If the response variable is continuous, supervised learning is referred to as regression; if the response  variable is categorical, with two or more categories, the task is referred to as classification.
While there are many different ways you could classify machine learning models, one that is useful for discussions of interpretability is to split models into three groups: i) parametric, statistical models, ii) non-parametric, statistical models and iii) non-statistical, non-parametric models (see box 1).
In this paper I define statistical models as being those fitted in a maximum likelihood or Bayesian framework and using a likelihood function such that the loss for any given prediction can be given as a probability.
If a model with a binomial likelihood predicts that, for a  given datapoint, 50\% of trials should be successes, and the observed data contain two out of two successes, the probability of this data, given the prediction, is 0.25.
Grounding these models in probability, and the inclusion of explicit noise terms, means that principled uncertainty distributions of both the parameters and predictions can be derived.
In contrast, I define non-statistical models as those that are fitted using non-likelihood based objective functions.
Often the only way to derive uncertainty intervals from these models is bootstrapping.

The focus on predictive performance, and the prevalence of non-parametric models in machine learning means that some concepts that are marginalised in typical ecological statistics become very important.
The first is out-of-sample performance.
With simple, parametric models, the goodness-of-fit of a model can be examined by looking at how far the data fall from the model predictions ($R^2$ of a regression for example).
However, with a non-parametric model, or a model with many covariates, you can easily fit a model that perfectly fits the data it was fitted to.
However, predictions from such a model will often be poor as the model was fitting to the noise in the data rather than extracting useful signal; this is termed overfitting.
To measure goodness-of-fit we need to test how well the model predicts data it has never seen before.
This out-of-sample performance can be tested by holding back a portion of the data, or by splitting the data into a $k$ chunks and fitting the model $k$ times each time holding out one chunk.
While out-of-sample performance is a measure of goodness-of-fit that is less affected by overfitting, we still need a way to make our models fit the signal rather than noise in the data.
This is achieved by regularization.
Regularization encompasses many techniques that forces the model to be simpler or fit less complicated relationships.
The strength of regularization is typically controlled by hyperparameters and often the only way to choose these parameters is to try a range of values and then select the values that provide the best out-of-sample performance.

\begin{figure*}[t]
\centering
\colorbox{boxcolour}{
\begin{minipage}{1.\textwidth}


\setlength{\parindent}{3mm}
\noindent\textsf{\textbf{Box 1: The classification of machine learning models used in this reivew}}\scriptsize 
\linespread{0.1}
\vspace{-4mm}
\paragraph{Parametric, statistical models}
This group includes many models commonly used by biologists.
They are parametric because their functional form (the shapes that the relationships between covariates and response variables can take) is defined in advance.
They are statistical because they will include some kind of likelihood function that makes the model probabilistic.
Therefore generalised linear models are included in this category.
A common technique to improve prediction is regularisation that biases parameter estimates (towards zero in the case of a linear model) to give a simpler model and avoid overfitting.
Methods for regularisation of linear models include the LASSO and other penalties \citep{tibshirani1996regression, zou2005regularization}, as used by maxent for example  \citep{maxent}. 
Other methods include stepwise selection \citep{hocking1976biometrics}, or Bayesian regularisation \citep{park2008bayesian, liu2018bayesian}.

\paragraph{Non-parametric, statistical models}
These models are fitted in a formal statistical framework as above but the functional form is not defined in advance.
Instead, flexible curves are fitted.
This group includes splines (and GAMs which combine splines and other linear terms) and Gaussian process regression \citep{rasmussen2004gaussian}.
These methods have principled uncertainty estimates due to being statistical.
Furthermore, while the non-parametric components are often not represented by a small number of interpretable parameters, they are often controlled, and regularized, by a small number of hyperparameters.
If these hyperparameters are fitted in a hierarchical framework (as is common) then they are can be interpreted with associated uncertainty.

\paragraph{Non-parametric, non-statistical models}
These methods encompass many more algorithmic methods \citep{crisci2012review} such as decision trees, ensembles of trees like Random Forests \citep{breiman2001random} and boosted regression trees \citep{elith2008working, friedman2001greedy}, neural networks \citep{neuralnets}, support vector machines \citep{svm} and nearest neighbour \citep{altman1992introduction}.
Each model has benefits but the variety of machine learning methods is reviewed elsewhere \citep{crisci2012review}.
These methods are not fully probabilistic, and often have large numbers of parameters that do not have uncertainty estimates.
Uncertainty estimates for predictions typically have to be computed using bootstrapping.
Regularization is achieved in these models in many different ways but in all cases the aim is the force the model to fit a smoother relationship between covariates and response values.

\end{minipage}

}
\normalcolor
\end{figure*}


A major shift in the statistical analysis of ecological and evolutionary data in recent decades is the acknowledgement that observational, biological data rarely conform to assumptions of independence due to phylogeny \citep{felsenstein1985phylogenies, ives2006statistics}, space \citep{redding2017evaluating, diggle1998model}, time \citep{ives2006statistics} or other categorical, grouping variables \citep{harrison2018brief, bolker2009generalized}.
This issue of autocorrelation is largely under-appreciated in the machine learning literature and only recently have random effects been explicitly built into typical machine learning models \citep{eo2014tree, hajjem2014mixed, hajjem2017generalized, miller2017gradient}.
Most machine learning models make some assumption of independence and estimates of out-of-sample predictive ability can be biased if cross-validation is used without accounting for autocorrelation.
There are however a number of strategies to mitigate biases caused by autocorrelation and for gaining insight into the random effects themselves.
These include simple methods such as using random effects as normal covariates or preprocessing the data to reduce autocorrelation \citep{elith2010art}.
These methods will be examined in more detail in the body of the review.

In this review I will present an illustrative analysis on the PanTHERIA dataset \citep{jones2009pantheria} which contains mammalian life history traits.
I fitted four models, with variations, that span the range of interpretability: i) a simple linear model with \emph{a priori} variable selection as is often used in ecology ii) a parametric statistical model, the elastic net \citep{elasticnet} iii) a non-parametric statistical model, Gaussian process regression \citep{rasmussen2004gaussian} and iv) a non-parametric, non-statistical model, Random Forest \citep{breiman2001random}.
For each of these models I demonstrate how they can be interpreted with methods that are applicable to a wide variety of machine learning models.
More specifically, I examine how the models can be interpreted at the level of the full model, the individual variable and at the level of the individual prediction.
I then demonstrate methods for handling and interpreting autocorrelation in the data.
The full analysis is included as a reproducible R \citep{R} script that reads data directly from online repositories (S1).

\section{Example analysis}\label{example-analysis}

\subsection{Data}\label{data}

The PanTHERIA database contains mammalian life history traits collected from the published literature \citep{jones2009pantheria}.
Overall it contains 5416 species and data on 35 traits, complimented by a further 15 variables calculated by intersecting IUCN shapefiles for each species and remotely sensed geographic data.
There are large amounts of missing data for many of the life history traits and these gaps were filled with median imputation as this method is both simple and conservative.
In this illustrative analysis I will use  this dataset to examine potential factors relating to the average litter size (with a \(\log(x+1)\) transform due to the strong left skew and presence of zeroes).
As each data row represents a species, the data are not independent; species with more recent common ancestors are likely to have similar life history traits.
Most analyses of this type of data \citep{felsenstein1985phylogenies, ives2006statistics, gay2014parasite, pellissier2012shifts, ferguson2014colony} would use phylogenetic regression which includes an estimated phylogeny, converted to a covariance matrix, as a random effect \citep{magnusson2017glmmtmb, caper}.
Methods for handling non-independence while using machine learning models are demonstrated in the section `Handling non-independent data'.

\subsection{Model fitting}\label{model-fitting}



I fitted four classes of model (with variations) to the data: a linear model with \emph{a priori} variable selection, a regularised linear model, a statistical, non-parametric Gaussian process model and a non-statistical Random Forest model.
I used five-fold cross-validation to test model accuracy and select hyperparameters.
Given the very different levels of flexibility in the models, this out-of-sample test of accuracy is important and given the non-statistical nature of the Random Forest, statistical, within-sample model comparisons such as AIC are not possible.
All models were fitted using \emph{caret} \citep{caret} in R \citep{R}.
One major benefit of \emph{caret} is that most of the procedures presented later for interpreting the models are immediately useable with over 200 machine learning models including up-to-date implementations of various models such as xgboost, h2o and keras \citep{xgboost, h2o, keras}.

\subsubsection{\texorpdfstring{\emph{A priori} variable selection}{A priori variable selection}}\label{a-priori-variable-selection}

The standard approach for modelling in ecology and comparative biology is to carefully select a relatively small set of covariates based on \emph{a priori} knowledge of the system \citep{whittingham2006we}.
This process ensures that all variables are reasonably likely to be causally important, reduces overfitting and keeps the number of parameters small.
As a baseline model, I fitted a linear model, selecting covariates that the literature suggests are related to litter size.
I chose body size \citep{leutenegger1979evolution, tuomi1980mammalian}, gestation length \citep{okkens1993influence, bielby2007fast}, metabolic rate \citep{white2004does}, litters per year \citep{white2004does} and longevity \citep{wilkinson2002life, zammuto1986life}.



\subsubsection{Statistical, parametric models}\label{statistical-parametric-models}

If we have many covariates relative to sample size and have minimal \emph{a priori} knowledge of the system we may wish to include all the covariates in a linear model but regularise the coefficients.
Similarly, if we want to include many interactions or transformed variables (as in maxent \citep{maxent} for example), the number of covariates can grow rapidly and regularisation becomes vital.
This approach is also sensible if we care more about prediction than about unbiased estimates of parameters.
The simplest regularised linear models are ridge regression \citep{ridge}, that includes a penalty on the square of the coefficients, and LASSO \citep{tibshirani1996regression} that penalises the absolute value of the coefficients and therefore more strongly penalises smaller values.
For the PanTHERIA analysis I fitted an elastic net, a common model that blends both the ridge and the LASSO penalty.
The total strength of the penalty, and the relative contribution of the two penalties were selected using cross-validation.
Figure \ref{fig:enethyp} shows how predictive performance varies with hyperparameter values and we can see that the best performing model had mostly a LASSO penalty and an intermediate amount of regularization.


\begin{figure}[t!]
  \centering
  \subfloat[Elastic net model\protect\label{fig:enethyp}]{\includegraphics[width =0.4\textwidth ]{figs/pub_figs_hyp-1.pdf}}  \qquad
  \subfloat[Gaussian process model\protect\label{fig:gphyp}]{\includegraphics[width = 0.4\textwidth]{figs/pub_figs_hyp-2.pdf}}

  \subfloat[Random Forest model\protect\label{fig:rfhyp}]{\includegraphics[width =0.4\textwidth ]{figs/pub_figs_hyp-3.pdf}}
  \label{fig:hyp}
  \caption{
    Model performance against hyperparameter values for each model. Each plot shows the hyperparameter(s) along the x-axis and with colour and model performance (bigger is better) on the y axis. A) the elastic net model with the proportion of  LASSO and ridge regularisation (0 is ridge regression, 1 is LASSO regression) shown on the x axis and the overall strength of the regularisation shown in color (yellow is strong regularization).  B) the Gaussian process model with the scale parameter sigma on the x axis (large values are smoother, more regularized relationships). C) the Random Forest model. mtry on the x axis determines the number of randomly chosen covariates considered for splits at each node (lower values give stronger regularization). The minimum node size (shown with color) controls the size of leaf nodes (higher values are stronger regularization).
  }
\end{figure}


\subsubsection{Non-parametric, statistical models}\label{non-parametric-statistical-models}

Given the parametric nature of the elastic net model, the way to include nonlinear responses and interactions is to define them manually before model fitting.
This however still imposes important restrictions as it is difficult to know which nonlinear functions are potentially useful and the model is still ultimately constrained by the effects we can think of to include (typically polynomial terms, log and exponential transforms and sine transforms).
In contrast, non-parametric models like Gaussian processes \citep{rasmussen2004gaussian} or splines \citep{splines} require no pre-specification of functional forms and instead the overall flexibility of the model is controlled with a hyperparameter.
Given their statistical nature, the uncertainty estimates around predictions are a natural part of the model and should be well calibrated even if we extrapolate far from the data.
For the PanTHERIA analysis I have fitted a Gaussian process model with a radial basis kernel \citep{kernlab}, selecting the scale hyperparameter using cross-validation.
We can see (figure \ref{fig:gphyp}) that a relatively short scale length, which allows quite non-linear relationships, had the best model performance.

\subsubsection{Non-parametric, non-statistical models}\label{non-parametric-non-statistical-models}

Finally, I fitted a Random Forest model \citep{breiman2001random, wright2015ranger} as an example of a non-statistical, non-parametric model.
I selected Random Forest as they tend to be easy to use, with few hyperparameters, and are robust to overfitting.
A Random Forest is an ensemble of decision trees with each tree being fitted to a random bootstrap sample of the input data and each split in each tree selecting from a random sample of the covariates.
Random Forests using the ranger  package \citep{wright2015ranger} via \emph{caret} have three hyperparameters.
Split rule, which determines how the decision tree splits are chosen, was set to `variance'.
The maximum number of data points at a leaf (min.node.size), which can be used to prevent overfitting was selected by cross-validation.
The number of randomly selected covariates to be considered at each split  was also selected by cross-validation.
We can see that high values of the number of randomly selected covariates and low values of minimum node size had the best performing hyperparameters (figure \ref{fig:rfhyp}).


\section{Global properties}\label{global-properties}

The first level at which we can interpret the model is the global level; what do the fitted models tell us about the system as a whole?
One global property of interest is how predictable the system is.
This can be assessed using scatter plots of observed versus out-of-sample predictions (Figure \ref{fig:enetpredobs} \ref{fig:gppredobs} \ref{fig:rfpredobs}) as well as metrics such as  \(r^2\) (the proportion of the variance of the held-out observed values explained by the predictions) or the root mean squared error (Table \ref{tbl:allr2}).
Random Forests are effective here as they are fast to fit, robust and need relatively little tuning.
If a Random Forest has poor predictive performance then it is likely that either vital covariates are missing from the dataset or that the response is in fact very noisy.
The Random Forest model fitted here has fairly good predictive performance (the points in figure \ref{fig:rfpredobs} cluster relatively closely to the one-one line) with an \(r^2\) of 0.68.
However, it can be seen that certain species, particularly those with very large litters, are predicted quite poorly.
We can be fairly sure that this trait is not noisy as the evolutionary consequences of litter size are large.
Therefore we are probably missing some important covariates which is a useful result gained solely by interpreting the model at the global level.

\begin{table}[t!]
\begin{longtable}[c]{@{}ll@{}}
\caption{\(R^2\) for all models fitted. \label{tbl:allr2}}\tabularnewline
\toprule
Model & \(R^2\)\tabularnewline
\midrule
\endfirsthead
\toprule
Model & \(R^2\)\tabularnewline
\midrule
\endhead
A priori linear & 0.34\tabularnewline
Elastic net & 0.53\tabularnewline
Gaussian Process & 0.63\tabularnewline
Random Forest & 0.68\tabularnewline
Random Forest w/ genus & 0.70\tabularnewline
A priori phylogenetic & 0.72\tabularnewline
Regularised phylogenetic & 0.74\tabularnewline
Stacked generalisation & 0.72\tabularnewline
Random Forest w/ phylogenetic distance & 0.81\tabularnewline
\bottomrule
\end{longtable}
\end{table}

We can also use predictive performance of machine learning models to scale our expectations for how well a more statistical or mechanistic model fits the data.
Here, the linear model with \emph{a priori} variable selection (Figure \ref{fig:aprioripredobs}) is unable to accurately model litter size and performed considerably worse than the other three models.
Furthermore, the Gaussian process model and Random Forest model performed considerably better than the elastic net model (Table \ref{tbl:allr2})
This suggests that some of the covariates included in the elastic net model, but not in the \emph{a priori} model were useful and that there are nonlinearities or interactions that are important but were not included in either the \emph{a priori} model or the elastic net model.
This is not a suggestion to go back and add these variables to our \emph{a priori} model.
This would amount to severe data snooping and would bias any significance tests performed on the \emph{a priori} model \citep{white2000reality}.
The suggestion of missing complexity in the \emph{a priori} model is evidence that the model is misspecified and therefore care should be taken when interpreting even this simple linear model \citep{lyddon2018nonparametric, maldonado1993interpreting}. 



\begin{figure}[t!]
  \centering
  \subfloat[\emph{a priori} model\label{fig:aprioripredobs}]{\includegraphics[width =0.5\textwidth ]{figs/pub_figs_cv-1.pdf}}
  \subfloat[Elastic net\label{fig:enetpredobs}]{\includegraphics[width = 0.5\textwidth]{figs/pub_figs_cv-2.pdf}}

  \subfloat[Gaussian process\label{fig:gppredobs}]{\includegraphics[width =0.5\textwidth ]{figs/pub_figs_cv-3.pdf}}
  \subfloat[Random Forest\label{fig:rfpredobs}]{\includegraphics[width = 0.5\textwidth]{figs/pub_figs_cv-4.pdf}}
  \label{fig:predobs}
  \caption{
    Predicted versus observed litter sizes for a) the \emph{a priori} selected model, b) the elastic net model, c) the Gaussian process model and d) the Random Forest model.
    The black line is the one-one line showing the correct predictions.
    The models are fitted under five fold cross-validation such that the data being predicted is not used at all during model fitting.
    We can clearly see that the \emph{a priori} model is not flexible enough to make good predictions resulting in artefacts such as many datapoints being predicted at very low values.
    We can also see in panel (d) that Random Forests are unable to extrapolate beyond the range of the data (no predictions are below the one-one line in the bottom left corner).
  }
\end{figure}

We can also attempt to interpret the hyperparameters of our models to try to understand something about the complexity of the system.
For the elastic net model, the lambda parameter and the number of non-zero coefficients give us some idea of the system's complexity (figure \ref{fig:enethyp}); if very few variables are retained and we get good predictive performance this suggests a simple system.
Here we have \(\lambda = 0.03\) as the selected hyperparameter and only one coefficient being forced to zero.
This gives some evidence that this is a complex system not easily explained by a few covariates.
Similarly, the length scale, \(\sigma\), in the Gaussian process model is a crude measure of complexity, with small values implying that the functional relationships are highly non linear (figure \ref{fig:gphyp}).

Finally, the Random Forest model has two hyperparameters (figure \ref{fig:rfhyp}); he number of randomly selected covariates to be examined at each split in each tree and the minimum node size i.e. the maximum number of data points that can be in a leaf node of a tree.
The minimum node size protects against overfitting and gives an indication of how much noise relative to signal there is.
Here, the smallest value of minimum node size tested gets elected which implies there is not much noise in the data relative to signal.
The selected value for the number of randomly selected covariates was 20.
The number of randomly selected covariates can be difficult to interpret and depends on the number of covariates included in the model.
Very small values imply little or no interactions between covariates while intermediate or high values indicate that there are interactions between covariates.
However, large values like the 20 selected here does not indicate interaction depths of 20.
Instead it more likely implies that there are many uninformative covariates and so 20 covariates are needed to avoid splits that consider no useful covariates.

\section{Variable level properties}\label{variable-level-properties}

The second level at which we can try to interpret the model is that of the individual covariate.
We can examine variable importance \citep{oppel2009alternative}, importance of interactions between pairs of covariates and start to examine the functional responses of covariates.
It is important however to remember that these models are not designed for inference; the following methods should be thought of as hypothesis generation and more formal, subsequent tests (on a different dataset) would be needed to confirm relationships between covariates and the response variable.

\begin{table}[t!]
\begin{longtable}[c]{@{}lll@{}}
\caption{Variable importance values. \label{tbl:varimp}}\tabularnewline
\toprule
Model & Variable & Importance\tabularnewline
\midrule
\endfirsthead
\toprule
Model & Variable & Importance\tabularnewline
\midrule
\endhead
Elastic net & GestationLen\_d & 100\tabularnewline
& AdultBodyMass\_g & 70.67\tabularnewline
& GR\_MidRangeLat\_dd & 67.42\tabularnewline
& GR\_MinLat\_dd & 62.73\tabularnewline
& PET\_Mean\_mm & 61.68\tabularnewline
Gaussian Process & GestationLen\_d & 100\tabularnewline
& AdultBodyMass\_g & 70.67\tabularnewline
& GR\_MidRangeLat\_dd & 67.42\tabularnewline
& GR\_MinLat\_dd & 62.73\tabularnewline
& PET\_Mean\_mm & 61.68\tabularnewline
Random Forest & GestationLen\_d & 100\tabularnewline
& AdultBodyMass\_g & 58.065\tabularnewline
& AdultForearmLen\_mm & 26.915\tabularnewline
& GR\_MidRangeLat\_dd & 25.800\tabularnewline
& PET\_Mean\_mm & 22.957\tabularnewline
\bottomrule
\end{longtable}
\end{table}

Table \ref{tbl:varimp} shows the top five most important variables as determined by the three models \citep{oppel2009alternative}.
These importance measures are not in absolute units so they are scaled such that the most important covariate has a value of 100.
For the regularised linear model, variable importance is given simply by the magnitude of the regression coefficients (i.e.~ignoring the sign) and these raw values might be more useful than the scaled importance values.
We can see that gestation length comes top for all three models and that latitude and potential evapotranspiration rate in the species' range (PET) are prominent in all three as well.
Fitting multiple models and searching for consistency is one useful way to increase confidence in results (as in \citet{appelhans2015evaluating}).
The fact that gestation length is found to be important also highlights the issue of causality; it is not clear which direction causality flows between gestation length and litter size.
Does large litter sizes force gestation length to be small or does short gestation length allow large litters? 
It could also be true that causality flows in different directions in different species.

\emph{Caret} provides an easy interface for getting variable importance measures for many model types; however the calculations being performed differ between models.
For some models, including Random Forest, there are different methods for calculating variable importance \citep{oppel2009alternative, seifert2019surrogate, basu2018iterative, wright2016little} and some are more correct than others, especially in the presence of categorical variables.
Here I have used the default variable importance given by \emph{caret}; given there are no categorical variables this is acceptably unbiased.
If more accurate variable importance measures are needed, a related model, conditional inference forests \citep{hothorn2006unbiased} or the maxstat split rule in ranger \citep{wright2017unbiased}, should be used instead.


\begin{figure}[t!]
  \centering
    \subfloat[Elastic net model\protect\label{fig:pdpgestenet}]{\includegraphics[width = 0.33\textwidth ]{figs/enet_marginals-1.pdf}}
  \subfloat[Gaussian process model\protect\label{fig:pdpgestgp}]{\includegraphics[width = 0.33\textwidth ]{figs/pdp_gest-1.pdf}}
  \subfloat[Random Forest model\protect\label{fig:pdpgestrf}]{\includegraphics[width = 0.33\textwidth]{figs/pdp_gest-2.pdf}}

  \label{fig:pdp}
  \caption{
    PDP plot for gestation length against log litter size in a) the elastic net model, b) the Gaussian process model and c) the Random Forest model.
    These plots show the predicted log litter size for each value of gestation length.
    Predictions are made at the covariate values of every datapoint, varying gestation length for each.
    The final curve is the mean of all $n$ predictions at each value of gestation length.
    The elastic net model can only fit linear relationships, the Gaussian process model fits smooth non-linear relationships while the Random Forest model fits more blocky relationships due to the decision trees that underpins it.
    All three models estimate a broadly decreasing trend.
    The Gaussian process model estimates a distinct U-shaped relationship while the Random Forest estimates a decreasing and then flat relationship.
  }
\end{figure}

It is also worth noting that the replicability of variable importance differs between model types and depends on the data set.
For example, repeatedly fitting a neural network to these data gives very different results each time.
In contrast, linear models, Gaussian processes and Random Forest generally give the same results each time.
Furthermore, variable importance in the presence of collinearity is less reliable and less interpretable \citep{dormann2013collinearity}.
Given two collinear variables, some models such as Random Forest will share the variable importance between them potentially masking an important variable.
In contrast, other models such as stepwise regression might put all the variable importance into one variable with no guarantee that the correct variable is selected.

Once some important covariates have been identified, it is useful to examine the shape of the relationship between covariate and response.
The simplest way to do this is a partial dependence plot (PDP) \citep{friedman2001greedy}.
This plot is created by evaluating the model $n \times m$ times, with all but one covariates taking their values in the $n$ datapoints  and the covariate of interested taking $m$ equally spaced values. 
The mean response for each value of the covariate of interest is then plotted.
For the regularised linear model all the responses are, by definition, linear so a PDP is not particularly useful but is included (Figure \ref{fig:pdpgestenet}) for reference and comparison.
The PDPs for gestation length for the Gaussian process and Random Forest models are shown in Figures \ref{fig:pdpgestgp} and \ref{fig:pdpgestrf}.
It can be seen that neither response is linear and are both decreasing for low values of gestation length.
However, the PDP for the Gaussian process model is increasing at high values of gestation length and is similar to a squared curve.
In contrast, the Random Forest model is flat at high values of gestation length.



While PDPs are computed as the mean of the response over the dataset, the variable importance measures calculated above are evaluated over all training data.
There can therefore be a mismatch where a PDP looks flat while the variable importance is high.
Relatedly, the PDP gives no information on interactions because only one curve is plotted.
To address these issues we can calculate the interaction importance for each covariate (table \ref{tbl:interimp}).
This value is given by decomposing the prediction function into contributions from just the focal covariate, contributions from everything except the focal covariate and contributions that rely on both the focal covariate and non-focal covariates together \citep{friedman2008predictive}.


Once we have identified covariates with important interactions we can use individual conditional expectation (ICE) plots.
Like PDPs, ICE plots calculate the predicted response value across a range of the focal covariate.
However, instead of averaging over the dataset, they plot one curve for each data point (Figure \ref{fig:icegestgp} and \ref{fig:icegestrf}).
In these plots we can start to see that the response curve differs depending on what value the other covariates take.
As the number of data points increases, these plots can get very busy and so clustering the curves is useful (figure \ref{fig:clusticelatgp} and \ref{fig:clusticelatrf}).
Here we can clearly see the range of responses that exist for a single covariate, with Gestation length having a negative relationship with litter size in many cases but a flatter relationship in others.


\begin{table}[t!]
\begin{longtable}[c]{@{}lll@{}}
\caption{Interaction strengths. \label{tbl:interimp}}\tabularnewline
\toprule
Model & Variable & Interaction Importance\tabularnewline
\midrule
\endfirsthead
\toprule
Model & Variable & Interaction Importance\tabularnewline
\midrule
\endhead
Gaussian Process & AdultBodyMass\_g & 0.20\tabularnewline
& AgeatEyeOpening\_d & 0.20\tabularnewline
& Terrestriality & 0.20\tabularnewline
& DispersalAge\_d & 0.16\tabularnewline
& MaxLongevity\_m & 0.11\tabularnewline
Random Forest & AdultBodyMass\_g & 0.34\tabularnewline
& GestationLen\_d & 0.23\tabularnewline
& Terrestriality & 0.15\tabularnewline
& GR\_MaxLat\_dd & 0.10\tabularnewline
& PET\_Mean\_mm & 0.10\tabularnewline
\bottomrule
\end{longtable}
\end{table}


Gaussian process models and Random Forests implicitly consider deep interactions which become increasingly difficult to interpret.
However, if we can identify important two way interactions we can start to interpret these.
We can find the interaction strength between two features in a similar fashion to finding variable importance.
We can consider the 2D PDP of two covariates (figures \ref{fig:2dgestlatgp} and  \ref{fig:2dgestlatrf}) and calculate what proportion of the curve is explained by the sum of the two 1D PDPs (e.g.~figure \ref{fig:pdpgestgp}).
We can therefore take one covariate that we know has strong interactions (``GestationLength\_d'' as seen in table \ref{tbl:interimp}) and calculate the two-way interaction strength between that covariate and all other covariates (table \ref{tbl:specificinter}).
Finally, once important interactions have been identified, the 2D PDP can be examined to determine the shape of that interaction (figure \ref{fig:2d}).
Looking at the 2D PDP of gestation length and PET for the Gaussian process model we can see that for most of gestation length we have a U-shaped relationship between PET and litter size.
However, at low values of gestation length, this U-shaped relationship flattens out and eventually becomes inverted.
In the Random Forest model, the relationship appears to be more monotonically decreasing with increased PET while there is again a suggestion of an inverse U-shape relationship at low values of gestation length.


\begin{table}[t!]
\begin{longtable}[c]{@{}lll@{}}
\caption{Specific interaction strengths between GestationLength\_d and other variables. \label{tbl:specificinter}}\tabularnewline
\toprule
Model & Variable & Interaction Importance\tabularnewline
\midrule
\endfirsthead
\toprule
Model & Variable & Interaction Importance\tabularnewline
\midrule
\endhead
Gaussian Process & WeaningBodyMass\_g\_EXT & 0.54\tabularnewline
& SocialGrpSize & 0.22\tabularnewline
& WeaningBodyMass\_g & 0.20\tabularnewline
& HomeRange\_km2 & 0.17\tabularnewline
& AgeatFirstBirth\_d & 0.16\tabularnewline
Random Forest & AdultForearmLen\_mm & 0.14\tabularnewline
& AdultBodyMass\_g & 0.12\tabularnewline
& WeaningAge\_d & 0.11\tabularnewline
& GR\_MidRangeLat\_dd & 0.08\tabularnewline
& Terrestriality & 0.07\tabularnewline
\bottomrule
\end{longtable}
\end{table}



\begin{figure}[t!]
  \centering
    \subfloat[Elastic net model\protect\label{fig:icegestenet}]{\includegraphics[width =0.33\textwidth ]{figs/enet_marginals-2.pdf}}
  \subfloat[Gaussian process model\protect\label{fig:icegestgp}]{\includegraphics[width =0.33\textwidth ]{figs/ice-1.pdf}}
  \subfloat[Random Forest model\protect\label{fig:icegestrf}]{\includegraphics[width = 0.33\textwidth]{figs/ice-2.pdf}}

  \subfloat[Gaussian process model\protect\label{fig:clusticelatgp}]{\includegraphics[width =0.5\textwidth ]{figs/clustered_ice-2.pdf}}
  \subfloat[Random Forest model\protect\label{fig:clusticelatrf}]{\includegraphics[width = 0.5\textwidth]{figs/clustered_ice-3.pdf}}

  \label{fig:ice}
  \caption{
    a-c) ICE plots for gestation length against log litter size in a) the elastic net model, b) the Gaussian process model and c) the Random Forest model.
    Each grey curve is the relationship between gestation length and litter size evaluated at one the datapoints (only 10\% of datapoints are plotted). 
    The yellow curves are the median of these curves and are therefore similar to PDPs as seen in Figure 3.
    The differences in the shapes of the curves are interaction effects; the relationship between gestation length and log litter size changes depending on the values of the other covariates.
    d-e) Clustered ICE plot for gestation length against normalised litter size in d) the Gaussian process model and e) the Random Forest model.
    When the number of datapoints used in the ICE plots gets large it becomes difficult to see individual curves.
    Clustered ICE plots are created by subtracting the mean of each curve and then clustering them using k-means.
    This normalisation step forces all the linear relationships in (a) to be identical and so the clustered plot is not shown. 
  }
\end{figure}


\begin{figure}[t!]
  \centering
    \subfloat[Elastic net model\protect\label{fig:2dgestlatenet}]{\includegraphics[width =0.33\textwidth ]{figs/enet_2d-1.pdf}}
  \subfloat[Gaussian process model\protect\label{fig:2dgestlatgp}]{\includegraphics[width =0.33\textwidth ]{figs/pdp_gest_pet-1.pdf}}
  \subfloat[Random Forest model\protect\label{fig:2dgestlatrf}]{\includegraphics[width = 0.33\textwidth]{figs/pdp_gest_pet-2.pdf}}
  \caption{
    2D PDP plot for gestation length and PET in a) the elastic net model, b) the Gaussian process model and c) the Random Forest model.
    These plots are created in the same way as the 1D PDP plots except now we are varying two covariates (gestation length and PET).
    As before we evaluate the models at a range of the values for the covariates of interest and at the covariate values from every datapoint and then take the mean.
    In the elastic net model we can see that the relationship is linear in with both covariates and the slopes do not change (i.e. there is no interaction).
    In the Gaussian process model we can see that for most values of gestation length we have a U-shaped relationship between PET and litter size.
    However, at low values of gestation length, this U-shaped relationship flattens out.
  }
  \label{fig:2d}
\end{figure}



\section{Data-point level properties}\label{data-point-level-properties}

\begin{figure}[t!]
\centering
    \subfloat[Gaussian process model\protect\label{fig:limegp}]{\includegraphics[width =0.7\textwidth ]{figs/lime2-1.pdf}}
    
  \subfloat[Random Forest model\protect\label{fig:limerf}]{\includegraphics[width =0.7\textwidth ]{figs/lime2-2.pdf}}
\caption{
LIME analysis of predictions of five points from the a) the Gaussian process model and b) the Random Forest model.
The analysis proceeds by taking a datapoint (the case number), permuting the covariate values around that point, evaluating the model at each permuted covariate value and then fitting a simple ridge regression model to this new dataset.
The explanation fit values is the $R^2$ of how well the simple model explains the predictions of the more complex models.
The absolute value of each bar indicates how important that variable is for this specific prediction.
Bars with show negative weights indicates that it is because the value of that covariate is small that high litter sizes are predicted.
The bars with positive weights indicate that it is because these values are large that high litter sizes are predicted.
\protect\label{fig:limegp}}
\end{figure}

The third level at which we can try to interpret models is that of the individual prediction \citep{lime, ribeiro2016should, lundberg2017unified, ribeiro2016nothing}.
Model interpretation at a single point is a much easier task than interpreting the global model because at a small enough scale the response curve is either flat or monotonically increasing or decreasing, so complex non-linear curves do not need to be considered.

However, it is difficult to examine the model at all data points.
Therefore we must focus our analysis on a few, interesting or important points.
Points with the highest or lowest predicted values may tell us something about what factors makes these points receive extreme predictions.
Alternatively, we might be more interested in a subset of points for an external reason.
In the PanTHERIA example we might be particularly interested in one taxonomic group.
Alternatively, we might want to interpret predictions that are going to be used directly in a conservation program for example.

The method Local Individual Model Evaluation (LIME) examines the behaviour of a model at a point by  permuting the covariates slightly around the point and making predictions from the model over this new dataset \citep{lime, ribeiro2016should, lundberg2017unified, ribeiro2016nothing}.
Then a simple, interpretable model, such as ridge regression, is fitted to this dataset.
As we do not need to consider non-linear relationships, this simpler model should accurately describe the behaviour at the local scale.

In figures \ref{fig:limegp} and \ref{fig:limerf} we can see the outputs of a LIME analysis for the two data points with the highest predicted litter size according to the Gaussian process and Random Forest model.
The simple model is a ridge regression model with the 10 covariates with highest weights being plotted.
The explanation fit values given in the plot is the $R^2$ of how well the simple, interpretable model explains the predictions of the more complex models.
In all cases here the simpler model explains around 60\% of the variation in the predictions of the permuted data.
It is important to note that as we know the true litter size values for these species we can see that the top-predicted data points are not actually the species with the highest litter size.
This reminds us not to interpret this LIME analysis as ``what factors imply the highest litter size'' but rather ``why are these particular species predicted as having large litters''.
We can however interpret these outputs alongside the variable importance estimates in table \ref{tbl:varimp}.


If we examine the species shown in the left pane of figure \ref{fig:limegp} we can see that dispersal age is the most important factor in determining why this species has been predicted as having a large litter size.
As the bars show negative weights this suggests that it is because the dispersal age of these species is small that they are predicted as having high litter sizes.
Indeed, both of these species are in the lowest 3\% of dispersal ages amongst the species in this data set.
Another way of phrasing this (that can be more useful for predictions in the centre of the range of response values) is that if the species had larger values for dispersal age, we would expect them to have smaller predicted litter sizes.
Similarly, the variables with positive weights indicate that these species have large predicted litter sizes because these covariates are large or that if these covariates were larger, the predicted litter sizes would be larger.



\section{Handling non-independent data}\label{handling-non-independent-data}

The PanTHERIA dataset is an example of a dataset that strongly violates assumptions of independence.
The autocorrelation here arises due to common ancestry of species; two species that recently diverged from a common ancestor are likely to be more similar than species whose common ancestor is in the deep past.
This autocorrelation is typically handled with a phylogenetic random effect \citep{felsenstein1985phylogenies, ives2006statistics, gay2014parasite, pellissier2012shifts, ferguson2014colony} while other sources of autocorrelation such as time or space can be similarly handled with an appropriate random effects term \citep{ives2006statistics, redding2017evaluating, diggle1998model}.
Categorical random effects can be used to model a wide variety of sources of autocorrelation such as multiple samples from a single individual, study site or lab  \citep{harrison2018brief, bolker2009generalized}.

Including random effects within parametric or non-parametric statistical models is entirely possible with flexible modelling packages such as Stan, INLA, TMB or Greta  \citep{stan, INLA, golding2019greta, tmb}.
As a simple demonstration I fitted a phylogenetic linear model with INLA \citep{INLA} using the \emph{a priori} selected covariates (\(r^2 = 0.72\)) and a phylogenetic linear model using all covariates and strong regularising priors (\(r^2 = 0.74\)).
For both models, the species-level phylogenetic terms correlate strongly with the species-level fitted value suggesting the phylogenetic effect is an important component of the models.
This interpretation is supported by the fact that the out-of-sample $r^2$ values are much higher in the phylogenetic models ($r^2 = 0.72$ and 0.74) than those of the non-phylogenetic models ($r^2 = 0.34$ and 0.53). 


However, combining random effects with non-parametric, non-statistical models is more difficult.
While these models are starting to be developed \citep{ngufor2019mixed, hajjem2014mixed, hajjem2017generalized, eo2014tree, miller2017gradient, REEMtree}, they are not available in R packages, are only implemented for a small subset of machine learning algorithms and do not necessarily benefit from the computational improvements implemented in the most up-to-date packages \citep{wright2015ranger, xgboost}.
Therefore, generic methods for handling random effects, that can be used with any machine learning algorithm, are useful.
The na\"{i}ve approach to including random effects within machine learning models would be to simply include them as covariates; categorical random effects as categorical covariates, space or time as continuous variables for example.
Interpretation of these variables could then be approached using the methods described in the section `variable level properties'.
However to understand when this approach is or is not appropriate, we have to examine three factors as to why these effects are not just included as fixed effects in typical mixed-effects models.

Firstly, we expect to extrapolate continuous random effects and expect unseen categories during prediction when using categorical random effects.
Many machine learning models extrapolate poorly, for example tree based models will predict a flat response curve outside the range of the data.
For an effect such as space this is undesirable and we would instead typically wish the spatial prediction to return to the mean of the data \citep{rasmussen2004gaussian, hengl2018random}.
Predicting unseen categories of a categorical variable presents problems as well.
A categorical variable might often be encoded as a full-rank dummy variable (one dummy variable less than the number of categories) and unseen categories would be implicitly predicted using the fitted value for the first category.
This is not how we would wish the model to behave.

Secondly, we often have many categories and little data per category in a categorical random effect and wish to share information across groups.
This low number of data points per parameter can be reframed as a regularisation problem.
The regularisation can be seen explicitly in the Bayesian formulation of random effects models (hierarchical models) where the random parameters are regularised by a zero centered prior, the strength of which is in turn learned from the data \citep{simpson2017penalising}.

Finally, random effects are often included as a way to control for autocorrelation rather than being part of the desired predictive model.
For example, if all future predictions are to be for unseen categories of a categorical random effect or if all spatial predictions are to be made far from data, then we might want to construct our model simply so that the model is unbiased by these autocorrelations rather than using them directly in predictions.
Similarly if the data collection was biased with respect to a random effect, we might want to control for this without wanting to use this effect in predictions.
For example, if data was collected by different labs or with different protocols, we might want to control for this effect but then predict the latent effect.
If the presence of a species is measured using different methods (camera trapping, visual surveys etc.) we might want to control for this, but we aim to predict the latent state ``species presence'', not ``species presence as measured by camera trapping''.


Given these issues we can consider how to include random effects into the Random Forest model and then examine the results when these are applied to the PanTHERIA analysis.
While we are continuing to use Random Forest as our example model, these methods are applicable to most machine learning methods.
One way of including phylogenetic information in an analysis is to treat a taxonomic level such as genus as a categorical random effect.
While this is less principled than properly including the phylogeny, it is simple.
This method also allows a demonstration of categorical random effects.

If we use genus as a categorical random effect to encapsulate some phylogenetic information, the first issue to is that we must be careful that the software does not automatically encode the data as a full-rank dummy variable.
While less-than-full-rank form would cause identifiability issues with the intercept in a linear model, the random columns and greedy splitting during tree building means we do not have to worry about identifiability in this context while using Random Forest.

The second issue above was that of regularisation.
Random Forest is natively regularised by the bootstrap aggregation, and the complexity of the model is further controlled by hyper parameters as in figure \ref{fig:rfhyp}.
The new model can therefore be fitted in the same way as the old model.
However, given that, in creating the dummy variables, I have added many covariates I increased the range of values considered for the number of randomly selected covariates.

The final consideration above was the case where we expect all predictions to be made on new categories.
In the case of Random Forest, the above methods are suitable.
However, given a model that cannot regularise as effectively or if our dataset was quite small we might have wanted to control for genus without including it as a covariate in the model at all.
In this case we can simply weight the data so that each genus is equally represented.
Alternatively we could weight the data so that each genus is represented proportionally to the number of species in each genus in the full prediction set (all mammals for example).
Many models in \emph{caret} accept a weight argument so this is a fairly general solution.

I obtained an \(r^2\) value of 0.70 for the model that used genus encoded as a  dummy variables.
As this is marginally better than the Random Forest model without genus as a covariate, this provides some evidence that phylogenetic effects are present.
The best hyperparameters were 500 for the number of randomly selected covariates  (which implies that many of the genera are not very useful on their own) and five for the minimum node size which is the same as the model without genus as a covariate.

If however, we wish to include the full phylogeny in our model, we need different methods.
The first method is to include all the phylogenetic information in covariates \citep{hengl2018random}.
Given the data set of 2143 data points we can do this by defining 2143 new covariates that measure the phylogenetic distance between data points.
That is, the first new covariate is the phylogenetic distance between every data point and the first datapoint.
This is then repeated to create 2143 new covariates.
This method is relatively new but is general and can work with any machine learning algorithm.
However, interpretation of the strength of the phylogeny will be difficult as it is encoded as 2143 different covariates.
Fitting a Random Forest to this augmented dataset was the best performing model out of all tested and gave an \(r^2\) of 0.81.

The second method involves fitting multiple machine learning models and then using phylogenetic regression to `stack' them.
We fit a number of machine learning algorithms and make out-of-sample predictions within the cross-validation framework.
We then fit a phylogenetic mixed-effects model using the out-of-sample predictions as covariates and constraining the regression coefficients to be positive.
This method is likely to be very effective at prediction and the phylogenetic component of the regression is interpretable as it would be in any normal phylogenetic regression.
However, this method only corrects for the biases from autocorrelated data after the machine learning models are fitted; while it may still be possible to interpret the machine learning models as we have done previously, the computed nonlinear relationships remain biased.
I fitted this model using the three original models (elastic net, Gaussian process regression and Random Forest) and fitted a hierarchical phylogenetic mixed-effects model using INLA \citep{INLA}.
I obtained a cross-validated \(r^2\) of 0.72.
Fitting the model on all the data yielded a posterior mean of 0.03 for the standard deviation of the phylogenetic random effect.

While I cannot demonstrate the handling of spatial or temporal autocorrelation with this dataset the methods described above are equally applicable \citep{elith2009species}.
In a method analogous to using genus as a categorical variable, space can be split into regions and the region used as a categorical variable \citep{appelhans2015evaluating}.
This approach is commonly used with predefined spatial units such as countries.
Another common approach with spatial data is ``thinning'' and is conceptually similar to the weighting method for categorical data \citep{elith2010art}.
In its simplest form, thinning, involves removing data points so that each spatial pixel has at most one data instance \citep{elith2010art, verbruggen2013improving}.
This is equivalent to treating the pixel as a categorical variable and subsampling until each pixel is equally represented (noting that each pixel is represented equally in the prediction dataset i.e.~once).
Also note that in the context of presence-only data, this is equivalent to weighting the data but with presence-absence data or continuous response data, weighting is a better way to include all the data.
More subtle methods involve removing data based on the local density \citep{verbruggen2013improving}.
In this method, a kernel bandwidth is chosen either \emph{a priori} or by cross-validation, then data is probabilistically removed based on the density of data geographically near it.
Again, weighting the data may be more principled.

Temporal effects are easier to handle as they are one dimensional with causation only able to occur in one direction.
Furthermore, they have been studied in detail in the machine learning literature \citep{jeong2008non}.
For regular time series we can typically include covariates created from the lagged response variable while for irregular time series we can create covariates like ``mean response within X units of time previous to this datapoint''.

\section{Software}

The accessibility to ecologists of machine learning models and methods for interpreting them is largely dependant on the availability of free, user-friendly software. 
Packages such as \emph{caret} and the more recent \emph{tidymodels} allow R users to fit many models within a unified interface \citep{caret, tidymodels}.
\emph{scikit-learn} provides a similar framework for python users.
These frameworks also provide functions for tuning hyperparameters, splitting data and other tasks vital for effective modelling.
The specific analyses and vizualisations are possible using a number of R packages such as \emph{iml, icebox} or \emph{pdp} for partial dependance plots and \emph{lime} for LIME analyses \citep{iml, lime, icebox, pdp}.
\emph{dalex}, available for python and R, provides pdp, ICE plots and LIME analyses \citep{dalex}.


\section{Future directions and conclusions}\label{future-directions-and-conclusions}

It is clear that machine learning is continuing to grow in popularity in ecology.
However, it currently remains used almost solely for purely predictive purposes.
The full potential of these methods is therefore not being realised.
For ecologists to get the most out of machine learning methods they must be more clear about the purposes of their analyses; is a well defined hypothesis being tested, is a dataset being explored for potential relationships to drive hypothesis generation, or is prediction the main focus?
This clarity makes it possible to be clear about the trade-offs in any statistical analysis and to use the most effective tools given the desired outcomes.
Using simple linear models is often not optimal if discovery of relationships or predictions are the aim; if a formal hypothesis is being tested Random Forests are unlikely to be the best choice.
Finally, being clear about the aims allows sensible planning on how data will be used in the longer term.
If the aim is to discover some relationships and then formally test them, the best use of a given dataset may be to split it and use half for discovery and half for hypothesis testing \citep{gelman2014statistical, nosek2012scientific}.
This workflow would not occur to an analyst who was unclear about their task.

While the methods here have been generic machine learning methods, there are a number of approaches for combining mechanistic models and non-parametric models.
These include using a mechanistic model as the mean function of a Gaussian process \citep{rasmussen2004gaussian} or using a mechanistic model as a regularising prior for a non-parametric model \citep{lyddon2018nonparametric}.
These methods have great potential for combining the interpretability of mechanistic models and the interpolative predictive ability of non-parametric machine learning models.

Finally, as with all modelling, interpretation of machine learning models requires human input.
While many algorithms are objectively tested for various modelling properties, very few have been tested for their ability to aid the human interpreter.
Studies that do specifically test this aspect are very welcome \citep{bastani2017interpreting}.
This algorithm-psychology interface is an important area of future research.

To conclude, in this review I have demonstrated  a number of methods for interpreting machine learning models.
These methods were used to examine global properties of the models, describe the ways in which individual variables relate to the response variable, quantify what factors drove individual predictions and finally to explore the autocorrelation structure within the data.
Interpreting machine learning models has two major benefits: firstly, predictions from a better understood model can be better defended in practical applications; secondly, machine learning methods can be used for exploratory statistics and hypothesis generation.
All models lie on a continuum of interpretability and complexity.
These models are not as easy to interpret as carefully constrained statistical models.
However this is because they instead allow greater predictive accuracy and allow the analyst to discover relationships they had no hypothesised \emph{a priori}.
The interpretation of these models needs to be done carefully and requires user interaction and exploration in a way that is less vital in interpreting standard statistical models.
To this end machine learning models tend to be better at exploration and hypothesis generation while more robust statistical methods are still needed to formally test most hypotheses.
The power here lies in the way a scientist defines their questions and uses machine learning alongside other methods.

 


\bibliography{machinelearn.bib}

\end{document}
